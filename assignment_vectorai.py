# -*- coding: utf-8 -*-
"""Assignment_VectorAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19C3vvfGWAiJEIdwgogTq6HZWFAVpnjhm
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential


# loading the dataset
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# explore the data
print('Train shape', train_images.shape)
print('Train Labels', train_labels)
print('Test shape', test_images.shape)
print('Test Labels', test_labels)

plt.figure()
plt.imshow(train_images[10])
plt.colorbar()
plt.show()

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(train_labels[i])
plt.show()

# Building a model
num_classes = 10

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, experimental

model = Sequential()
model.add(experimental.preprocessing.Rescaling(1./255, input_shape=(28, 28, 1)))
model.add(Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
model.add(Dense(num_classes, activation='softmax'))

# compile a model
from tensorflow.keras.optimizers import SGD

opt = SGD(lr=0.01, momentum=0.9)
model.compile(optimizer=opt,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train model
epochs=20
model.fit(train_images, train_labels, epochs=epochs)

# evaluate model
test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

"""As we can see that the test accuracy is pretty less than training acuracy This mismatch between the accuarcy occurs when model is overfitting the training data. To mitigate this gap one can introduce various startegies. One such strategy is Dropout. """

# Adding Dropout layer to furher push the accuracy of test data
from tensorflow.keras.layers import Dropout

model = Sequential()
model.add(experimental.preprocessing.Rescaling(1./255, input_shape=(28, 28, 1)))
model.add(Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
model.add(Dropout(0.4))
model.add(Dense(num_classes, activation='softmax'))

opt = SGD(lr=0.01, momentum=0.9)
model.compile(optimizer=opt,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=epochs)

# evaluating the model on test data
test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

"""From the above accuracy we can see that we successfully reduced the gap between train and test accuracy by introducing Dropout."""

# save model
model.save('saved_model.h5')

from google.colab import drive
drive.mount('/content/drive')

# Save the entire model as a SavedModel.
!mkdir -p saved_model

path_to_dir = '/content/drive/MyDrive/'
tf.saved_model.save(model, path_to_dir)

from joblib import dump
dump(model, './cnn_model.joblib')

# to load the model
model = tf.saved_model.load(path_to_dir)

